# -*- coding: utf-8 -*-
"""modulte_absenteeism.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16j99xM8FFmwiDHC7AenHh5PrRhZS_Y6V
"""

import pandas as pd
import numpy as np
import pickle
from sklearn.preprocessing import StandardScaler

class Absenteeism_model():
  def __init__(self, model_file, scaler_file):
    with open('Model_portofolio', 'rb') as model_file, open('Scaler_portofolio', 'rb') as scaler_file:
      self.regressor = pickle.load(model_file)
      self.scaler = pickle.load(scaler_file)
      self.data_final = None
  
  def load_and_cleaning_data(self, data_file):
    #load the data
    dataraw = pd.read_csv(data_file, delimiter=',')
    df = dataraw.copy()
    self.data_for_tabel = df.copy()
    #drop the ID column
    data = df.drop(['ID'], axis=1)
    #dealing with time series columns
    #fixing the date format
    data['Date'] = pd.to_datetime(data['Date'], format = '%d/%m/%Y')
    list_day = []
    list_month = []
    list_year = []
    for x in range(len(data.Date)):
      list_month.append(data['Date'][x].month)
      list_day.append(data['Date'][x].weekday())
      list_year.append(data['Date'][x].year)
    #since we know that the weekday doesn't give us a good significance, we decide to sparse
    #the date format, only for year and month only
    data['weekday'] = list_day
    data['month'] = list_month
    data['year'] = list_year
    #print(data['year'].value_counts())
    data = data.drop(['Date'], axis=1)
    #since we known that the sixth day is sunday, where the employes, were shouldn't go for work
    #then we consider to drop the rows with 6 values in days column
    data = data.drop((data.loc[(data['weekday'] >= 5) & (data['weekday'] <= 6)]).index, axis=0)
    #then, since we know that the variable days, doesn't give us a good significancy, we consider to
    #drop it immediately
    #since the month variable have 12 values, we consider to reduce this hig cardinality by
    #splitting this 12 values into 3 Quarter.
    data['weekday'] = data['weekday'].map({0:'monday', 1:'tuesday', 2:'wednesday', 3:'thursday', 4:'friday'})
    #data['month'] = data['month'].map({1:'Q1',2:'Q1',3:'Q1',4:'Q1',5:'Q2',6:'Q2',7:'Q2',8:'Q2',9:'Q3',10:'Q3',11:'Q3',12:'Q3'})
    #data['month'].value_counts()
    #print(data.head())
    weekday_dummies = pd.get_dummies(data['weekday'], drop_first=True)
    date = pd.concat([weekday_dummies], axis=1)
    #data = pd.get_dummies(data, columns=['month','year','weekday'], drop_first=True)
    checkpoint1 = data
    #print(data.head(3))
    #dealing with 'Reason for Absence' column
    data2 = checkpoint1.copy()
    data_reason = pd.get_dummies(data2['Reason for Absence'], drop_first=True)
    reason1 = data_reason.loc[:,1:14].sum(axis=1)
    reason2 = data_reason.loc[:,15:17].sum(axis=1)
    reason3 = data_reason.loc[:,18:21].sum(axis=1)
    reason4 = data_reason.loc[:,22:].sum(axis=1)
    reason = pd.concat([reason1, reason2, reason3, reason4], axis=1)
    reason.columns = ['reason1', 'reason2', 'reason3', 'reason4']
    data_reason_preprocessed = pd.concat([reason, data2.drop(['Reason for Absence','weekday','month','year'], axis=1), date], axis=1)
    self.preprocessed_data = data_reason_preprocessed.copy()
    #move_column = data_reason_preprocessed.pop('Absenteeism Time in Hours')
    #data_reason_preprocessed.insert(data_reason_preprocessed.shape[1], ['Absenteeism Time in Hours'], move_column)
    checkpoint2 = data_reason_preprocessed.copy()
    data_standard_process = checkpoint2.copy()
    data_standard_process.iloc[:,4:12] = self.scaler.transform(data_standard_process.iloc[:,4:12])
    checkpoint3 = data_standard_process.copy()
    self.data_final = checkpoint3.copy()
    self.data_final.to_csv('data_preprocessed.csv', index=False)
  
  def predicted_probability(self):
    if (self.data_final is not None):  
      pred = self.regressor.predict_proba(self.data_final)[:,1]
      return pred
    
  def predicted_output_category(self):
    if (self.data_final is not None):
      pred_outputs = self.regressor.predict(self.data_final)
      return pred_outputs
  
        # predict the outputs and the probabilities and 
        # add columns with these values at the end of the new data
  def predicted_outputs(self):
    if (self.data_final is not None):
      self.data_for_tabel['Probability'] = self.regressor.predict_proba(self.data_final)[:,1]
      self.data_for_tabel ['Prediction'] = self.regressor.predict(self.data_final)
      self.data_for_tabel.to_csv('predictions_portofolio.csv', index=False)
      return self.data_for_tabel